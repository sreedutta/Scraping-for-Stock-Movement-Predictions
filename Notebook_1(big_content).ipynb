{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFoJHXYB0tyFhiF/njc86g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrutakeerti/Scraping-for-Stock-Movement-Predictions/blob/main/Notebook_1(big_content).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Install Necessary Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "h9dW8zzl6Now"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZRLSlw5sNWP",
        "outputId": "637d151f-26cd-4b13-81b4-0880cc17dd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw pandas scikit-learn vaderSentiment matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Scraping (PRAW for Reddit)"
      ],
      "metadata": {
        "id": "j64ytMpO6RAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize Reddit instance\n",
        "reddit = praw.Reddit(client_id='zBslG12V_uTtmH1K8ieYSQ',\n",
        "                     client_secret='d0IUvqDAK7Z1plYz0Vr4MBWHK2pgsg',\n",
        "                     user_agent='TaeTaeBot:v1.0 (by /u/Equivalent_Let_8310)')\n",
        "\n",
        "# Scrape Reddit data from the 'stocks' subreddit\n",
        "subreddit = reddit.subreddit('stocks')\n",
        "\n",
        "def scrape_reddit_data(limit=500):\n",
        "    posts = []\n",
        "    for submission in subreddit.new(limit=limit):\n",
        "        posts.append([submission.title, submission.selftext, submission.score, submission.num_comments, submission.created])\n",
        "    return pd.DataFrame(posts, columns=['Title', 'Body', 'Upvotes', 'Comments', 'Created_At'])\n",
        "\n",
        "reddit_data = scrape_reddit_data(1000)\n",
        "\n",
        "# Save raw scraped data\n",
        "reddit_data.to_csv('reddit_stock_data.csv', index=False)\n",
        "print(\"Scraped Reddit data successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTN8FC6ytrj7",
        "outputId": "2f8153c3-7bf8-4c6e-cd10-cdefe3bf2a60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped Reddit data successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis Using Pretrained BERT (FinBERT)"
      ],
      "metadata": {
        "id": "6oIAAAi56Vmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "import torch\n",
        "\n",
        "# Load pre-trained FinBERT model for sentiment analysis\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "def get_finbert_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    probs = softmax(outputs.logits, dim=-1)\n",
        "    sentiment = torch.argmax(probs).item()\n",
        "    return sentiment  # 0: Negative, 1: Neutral, 2: Positive\n",
        "\n",
        "# Apply FinBERT sentiment analysis to the Reddit posts\n",
        "reddit_data['Content'] = reddit_data['Title'] + ' ' + reddit_data['Body']\n",
        "reddit_data['Sentiment'] = reddit_data['Content'].apply(get_finbert_sentiment)\n",
        "\n",
        "# Map sentiment to string labels for interpretability\n",
        "reddit_data['Sentiment_Label'] = reddit_data['Sentiment'].map({0: 'Negative', 1: 'Neutral', 2: 'Positive'})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SINeW2GbuQUe",
        "outputId": "4e516de5-3a53-4481-8e38-a71187d6d530"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "-eoKvsyo6cd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Adding basic features\n",
        "reddit_data['Post_Length'] = reddit_data['Content'].apply(len)  # Post length (characters)\n",
        "reddit_data['Upvote_Comment_Ratio'] = np.where(reddit_data['Comments'] == 0, reddit_data['Upvotes'], reddit_data['Upvotes'] / reddit_data['Comments'])  # Upvotes to comment ratio\n",
        "\n",
        "# Example stock symbol mentions (add your own relevant symbols)\n",
        "symbols = ['AAPL', 'TSLA', 'AMZN', 'GOOGL', 'MSFT']\n",
        "for symbol in symbols:\n",
        "    reddit_data[f'Mention_{symbol}'] = reddit_data['Content'].apply(lambda x: 1 if symbol in x else 0)\n",
        "\n",
        "# Save preprocessed data for easy use\n",
        "reddit_data.to_csv('preprocessed_stock_data_with_bert.csv', index=False)\n"
      ],
      "metadata": {
        "id": "QQPWyLlxyyMU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training with XGBoost"
      ],
      "metadata": {
        "id": "rvAK-oJ26hpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv('preprocessed_stock_data_with_bert.csv')\n",
        "\n",
        "# Target variable (example: stock movement; this depends on how you're defining movement)\n",
        "data['Stock_Movement'] = data['Upvotes'].apply(lambda x: 1 if x > 100 else 0)  # Adjust threshold as per your dataset\n",
        "\n",
        "# Define features and target\n",
        "features = ['Sentiment', 'Post_Length', 'Upvote_Comment_Ratio'] + [f'Mention_{symbol}' for symbol in symbols]\n",
        "X = data[features]\n",
        "y = data['Stock_Movement']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "xgb_model = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgeL137ty5Y6",
        "outputId": "77d8ee8c-79e3-4a25-fc7f-41c68211bd16"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.7634\n",
            "Precision: 0.5789\n",
            "Recall: 0.5641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning for XGBoost"
      ],
      "metadata": {
        "id": "IZ0N2Jb66nBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1],\n",
        "    'colsample_bytree': [0.8, 1]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions with the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate best model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "precision_best = precision_score(y_test, y_pred_best)\n",
        "recall_best = recall_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"Optimized Model Accuracy: {accuracy_best:.4f}\")\n",
        "print(f\"Optimized Precision: {precision_best:.4f}\")\n",
        "print(f\"Optimized Recall: {recall_best:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scYd65UvzFoI",
        "outputId": "d3c4ce12-6ee4-4668-a9da-22bc62470f94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Optimized Model Accuracy: 0.7993\n",
            "Optimized Precision: 0.6667\n",
            "Optimized Recall: 0.5641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L5IZOauUNFYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Define stock symbols\n",
        "symbols = ['AAPL', 'TSLA', 'AMZN', 'GOOGL', 'MSFT']\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv('preprocessed_stock_data_with_bert.csv')\n",
        "\n",
        "# Target variable\n",
        "data['Stock_Movement'] = data['Upvotes'].apply(lambda x: 1 if x > 100 else 0)\n",
        "\n",
        "# Define features and target\n",
        "features = ['Sentiment', 'Post_Length', 'Upvote_Comment_Ratio'] + [f'Mention_{symbol}' for symbol in symbols]\n",
        "X = data[features]\n",
        "y = data['Stock_Movement']\n",
        "\n",
        "# Handle class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize individual models\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "lgb_model = LGBMClassifier()\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Create a voting classifier\n",
        "voting_classifier = VotingClassifier(estimators=[\n",
        "    ('xgb', xgb_model),\n",
        "    ('lgb', lgb_model),\n",
        "    ('rf', rf_model)],\n",
        "    voting='soft'  # Use soft voting\n",
        ")\n",
        "\n",
        "# Fit the voting classifier\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_voting = voting_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
        "precision_voting = precision_score(y_test, y_pred_voting)\n",
        "recall_voting = recall_score(y_test, y_pred_voting)\n",
        "f1_voting = f1_score(y_test, y_pred_voting)\n",
        "\n",
        "print(f\"Voting Classifier Accuracy: {accuracy_voting:.4f}\")\n",
        "print(f\"Voting Classifier Precision: {precision_voting:.4f}\")\n",
        "print(f\"Voting Classifier Recall: {recall_voting:.4f}\")\n",
        "print(f\"Voting Classifier F1 Score: {f1_voting:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M51-ec_kcIl5",
        "outputId": "9b08db5b-741b-4c5c-ce86-6ddaff2fba4b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:16:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 468, number of negative: 458\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 513\n",
            "[LightGBM] [Info] Number of data points in the train set: 926, number of used features: 3\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505400 -> initscore=0.021599\n",
            "[LightGBM] [Info] Start training from score 0.021599\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Voting Classifier Accuracy: 0.8719\n",
            "Voting Classifier Precision: 0.8265\n",
            "Voting Classifier Recall: 0.9330\n",
            "Voting Classifier F1 Score: 0.8765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBOqhIEMc4KI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}